{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AI Data Engineering Agent (Master Edition)\n",
                "\n",
                "This is a **True Full-Scan Agent** that meets the 10-Point Master Checklist.\n",
                "It does NOT just summarize data. It reads **every single row** (in chunks), detects hidden issues (like 'Delhi#2024'), writes production-grade code, executes it, and hands you a finished notebook.\n",
                "\n",
                "## 1. Setup & Dependencies\n",
                "Ensure you have the required packages installed:\n",
                "`!pip install langchain langchain-google-genai pandas numpy sqlalchemy joblib scikit-learn nbformat`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import json\n",
                "import joblib\n",
                "import nbformat as nbf\n",
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from langchain.prompts import ChatPromptTemplate\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "# Please set your GOOGLE API key here (Get it from aistudio.google.com)\n",
                "os.environ[\"GOOGLE_API_KEY\"] = \"AIza...\" \n",
                "\n",
                "# Using Gemini Pro for its large context window (Critical for accumulating scan insights)\n",
                "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Module 1: The Chunk Scanner (Deep Awareness)\n",
                "This function iterates through the dataset in chunks, converting rows to JSON. \n",
                "It asks Gemini to identify specific defects in each chunk that summary stats would miss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scan_dataset(df, chunk_size=300):\n",
                "    \"\"\"\n",
                "    Scans the ENTIRE dataset in chunks of `chunk_size`.\n",
                "    Returns a aggregated list of issues found by Gemini.\n",
                "    \"\"\"\n",
                "    print(f\"Starting Full-Scan on {len(df)} rows...\")\n",
                "    \n",
                "    all_issues = []\n",
                "    \n",
                "    chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
                "    \n",
                "    prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
                "    Analyze this snippet of raw data rows (JSON format):\n",
                "    {data_chunk}\n",
                "    \n",
                "    Identify SPECIFIC data quality issues. Look for:\n",
                "    1. Mixed types in the same column (e.g. strings in numeric cols).\n",
                "    2. Hidden delimiters (e.g. 'Delhi#2024' or 'John|Doe').\n",
                "    3. Dirty formats (Date variations like '12-01-2020' vs 'Jan 12, 2020').\n",
                "    4. Outliers or impossible values (Age = 150).\n",
                "    \n",
                "    Return ONLY a concise bulleted list of issues found. If none, say 'No major issues'.\n",
                "    \"\"\")\n",
                "    \n",
                "    for i, chunk in enumerate(chunks):\n",
                "        # Convert chunk to JSON for LLM readability\n",
                "        chunk_json = chunk.to_json(orient='records')\n",
                "        \n",
                "        # FULL SCAN: No Break. We iterate everything.\n",
                "        # Checklist Item #1: \"The agent must read the ENTIRE dataset\"\n",
                "            \n",
                "        print(f\"Scanning Chunk {i+1}/{len(chunks)}...\")\n",
                "        chain = prompt_template | llm\n",
                "        result = chain.invoke({\"data_chunk\": chunk_json})\n",
                "        \n",
                "        if \"No major issues\" not in result.content:\n",
                "            all_issues.append(f\"Chunk {i+1} Issues:\\n{result.content}\")\n",
                "            \n",
                "    print(\"Scan Complete.\")\n",
                "    return \"\\n\".join(all_issues)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Module 2: The Code Architect (Pipeline Generation)\n",
                "Takes the scan report and basic dataframe info to write the Cleaning Code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_cleaning_code(df, scan_report):\n",
                "    \"\"\"\n",
                "    Generates executable Python code to fix the issues identified in the scan.\n",
                "    \"\"\"\n",
                "    print(\"Architecting Data Pipeline...\")\n",
                "    \n",
                "    dtypes_info = df.dtypes.to_string()\n",
                "    head_info = df.head().to_string()\n",
                "    \n",
                "    prompt = f\"\"\"\n",
                "    You are a Senior Data Engineer. \n",
                "    \n",
                "    I have scanned the dataset and found these issues:\n",
                "    {scan_report}\n",
                "    \n",
                "    Dataset Head:\n",
                "    {head_info}\n",
                "    \n",
                "    Types:\n",
                "    {dtypes_info}\n",
                "    \n",
                "    TASKS:\n",
                "    1. Create a python script to CLEAN this dataset.\n",
                "    2. Handle missing values (Impute numericals with mean/median, categoricals with mode).\n",
                "    3. Fix identified issues (e.g. split columns if needed, convert types).\n",
                "    4. Encode Categoricals (LabelEncoder) and Scale Numericals (StandardScaler).\n",
                "    5. SAVE artifacts: 'clean_data.csv', 'model.pkl' (dummy), 'preprocessor.pkl'.\n",
                "    \n",
                "    CRITICAL RULES:\n",
                "    - The code must assume `df` is already loaded.\n",
                "    - Do NOT re-load the data.\n",
                "    - Use `joblib` to save the scaler/encoders.\n",
                "    - Return ONLY the python code block wrapped in ```python ... ```.\n",
                "    \"\"\"\n",
                "    \n",
                "    result = llm.invoke(prompt)\n",
                "    return result.content"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Module 3: In-Memory Execution\n",
                "Runs the generated code using `exec()` to modify the dataframe in real-time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def execute_pipeline(df, code):\n",
                "    \"\"\"\n",
                "    Executes the cleaning code on the DataFrame.\n",
                "    \"\"\"\n",
                "    import re\n",
                "    \n",
                "    # Extract code info\n",
                "    code_match = re.search(r\"```python(.*?)```\", code, re.DOTALL)\n",
                "    if code_match:\n",
                "        clean_code = code_match.group(1)\n",
                "    else:\n",
                "        clean_code = code\n",
                "        \n",
                "    print(\"--- Executing Generated Pipeline ---\")\n",
                "    print(clean_code)\n",
                "    print(\"------------------------------------\")\n",
                "    \n",
                "    # Prepare execution environment\n",
                "    # We pass 'df' into the exec scope so the code can modify it.\n",
                "    exec_globals = globals().copy()\n",
                "    exec_globals['df'] = df\n",
                "    exec_globals['pd'] = pd\n",
                "    exec_globals['np'] = np\n",
                "    exec_globals['joblib'] = joblib\n",
                "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "    exec_globals['StandardScaler'] = StandardScaler\n",
                "    exec_globals['LabelEncoder'] = LabelEncoder\n",
                "    \n",
                "    try:\n",
                "        exec(clean_code, exec_globals)\n",
                "        print(\"\\nExecution Success! Artifacts (clean_data.csv, etc.) should be saved.\")\n",
                "        # Retrieve modified df if needed, though clean_data.csv is the source of truth now\n",
                "        return True\n",
                "    except Exception as e:\n",
                "        print(f\"EXECUTION FAILED: {e}\")\n",
                "        return False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Module 4: The Handover (User Notebook Gen)\n",
                "Creates a ready-to-use notebook for the user, loading the *cleaned* data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_user_notebook():\n",
                "    nb = nbf.v4.new_notebook()\n",
                "    \n",
                "    nb.cells.append(nbf.v4.new_markdown_cell(\"# AI Engineered Data Notebook\\nThis notebook loads your professionally cleaned dataset.\"))\n",
                "    \n",
                "    code = \"\"\"\n",
                "import pandas as pd\n",
                "import joblib\n",
                "\n",
                "# 1. Load Cleaned Data\n",
                "df = pd.read_csv('clean_data.csv')\n",
                "print('Loaded Cleaned Data:', df.shape)\n",
                "\n",
                "# 2. Load Preprocessors\n",
                "try:\n",
                "    preprocessor = joblib.load('preprocessor.pkl')\n",
                "    print('Loaded Preprocessor:', preprocessor)\n",
                "except:\n",
                "    print('No preprocessor found (maybe none were needed).')\n",
                "\n",
                "display(df.head())\n",
                "\"\"\"\n",
                "    nb.cells.append(nbf.v4.new_code_cell(code))\n",
                "    \n",
                "    nb.cells.append(nbf.v4.new_markdown_cell(\"## Next Steps\\nYou can now run your ML models below.\"))\n",
                "    nb.cells.append(nbf.v4.new_code_cell(\"# Your Model Here\\n# from sklearn.ensemble import RandomForestClassifier...\"))\n",
                "    \n",
                "    with open('AI_Data_Engineered_Notebook.ipynb', 'w') as f:\n",
                "        nbf.write(nb, f)\n",
                "    print(\"Created 'AI_Data_Engineered_Notebook.ipynb' for the user.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. The Master Agent Runner\n",
                "Orchestrates the entire flow."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_data_engineering_agent(df):\n",
                "    if df is None:\n",
                "        return \"No data.\"\n",
                "    \n",
                "    # 1. Scan\n",
                "    scan_report = scan_dataset(df)\n",
                "    \n",
                "    # 2. Architect\n",
                "    code = generate_cleaning_code(df, scan_report)\n",
                "    \n",
                "    # 3. Execute\n",
                "    success = execute_pipeline(df, code)\n",
                "    \n",
                "    # 4. Handover\n",
                "    if success:\n",
                "        create_user_notebook()\n",
                "        print(\"Done. Open 'AI_Data_Engineered_Notebook.ipynb' to continue.\")\n",
                "    else:\n",
                "        print(\"Failed to create user notebook due to execution errors.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Test Drive\n",
                "Let's create a nasty dataset with hidden issues."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dirty Data\n",
                "data = {\n",
                "    'ID': [1, 2, 3, 4, 5],\n",
                "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
                "    'Age': [25, 30, 150, 40, 22], # 150 is outlier\n",
                "    'City_Year': ['NY#2021', 'LA#2020', 'SF#2022', 'NY#2021', 'Austin#2023'], # Hidden Delimiter\n",
                "    'Salary': ['50k', '60000', '75k', '90000', 'None'] # Mixed types and strings\n",
                "}\n",
                "df_dirty = pd.DataFrame(data)\n",
                "\n",
                "print(\"Original Dirty Data:\")\n",
                "print(df_dirty)\n",
                "\n",
                "# Go!\n",
                "run_data_engineering_agent(df_dirty)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
